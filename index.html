<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Chapter: Transformer Blocks ‚Äî Feedforward and Layer Normalization</title>
  <style>
    body {font-family: system-ui, -apple-system, Segoe UI, Roboto, "Helvetica Neue", Arial; line-height:1.6; color:#111; padding:28px; max-width:900px; margin:0 auto;}
    h1,h2,h3 {color:#0b3d91}
    pre {background:#f6f8fa; border:1px solid #e1e4e8; padding:12px; overflow:auto}
    .box {border-left:4px solid #0b3d91; padding:12px; background:#f8fbff}
    .eq {font-family: "Times New Roman", Times, serif; font-size:1.05em}
    svg {max-width:100%; height:auto}
    .note {background:#fff6e6;border:1px solid #ffd8a8;padding:10px;margin:10px 0}
    table {border-collapse:collapse;margin:12px 0}
    td,th {border:1px solid #ddd;padding:8px}
    code {background:#f1f1f1;padding:2px 6px;border-radius:4px}
p {
  margin: 0; 
  text-align: justify;  /* aligns text to both left and right edges */
  text-justify: inter-word; /* improves spacing in some browsers */
}

p, h1, h2, h3, ul, ol, .equation, figure, figcaption {
  margin: 0 auto;
  text-align: justify;
  text-justify: inter-word;
}


  </style>
  
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>
<body>
  <div class="container">
    <header>
      <h1>Transformers</h1>
    </header>

    <section>
      <h1>A. Introduction to the Transformer</h1>
      <p>
        The <strong>transformer</strong>,is the standard architecture behind today‚Äôs large language models (LLMs). Transformers have completely changed the way we do speech and language processing, and every chapter that follows will build on this idea.
      </p>
    </section>

    <section>
      <h2>What is a Transformer?</h2>
      <p>
        A transformer is a <strong>neural network</strong> with a special structure that uses <strong>self-attention (or multi-head attention)</strong>.
      </p>
      <ul>
        <li><strong>Attention</strong> is a way for the model to decide which other words (tokens) are important when figuring out the meaning of the current word.</li>
        <li>This allows the model to capture relationships between tokens even when they are far apart in the sentence.</li>
      </ul>
    </section>

    <section>
  <h2>Transformer Architecture</h2>
  <p>The figure below shows the main components of the transformer:</p>
  <img src="./images/transformer-architecture.png" 
       alt="Diagram of transformer architecture" 
       width="700">

      <p>The transformer has <strong>three main parts</strong>:</p>

      <h3>1. Input Encoding</h3>
      <ul>
        <li>Each input token (like the word <span class="code">thanks</span>) is turned into a vector using an <strong>embedding matrix (E)</strong>.</li>
        <li>The model also adds <strong>position information</strong>, so it knows the order of tokens.</li>
      </ul>

      <h3>2. Transformer Blocks (the core of the model)</h3>
      <ul>
        <li>These are stacked in <strong>columns</strong>, often 12‚Äì96 layers deep.</li>
        <li>Each block contains:
          <ul>
            <li><strong>Multi-head attention</strong> (looks at other tokens for context)</li>
            <li><strong>Feedforward networks</strong> (process the information further)</li>
            <li><strong>Layer normalization</strong> (keeps training stable)</li>
          </ul>
        </li>
        <li>Together, the blocks transform the sequence of input vectors <span class="code">(x<sub>1</sub>, ‚Ä¶, x<sub>n</sub>)</span> into output vectors <span class="code">(h<sub>1</sub>, ‚Ä¶, h<sub>n</sub>)</span>.</li>
      </ul>

      <h3>3. Language Modeling Head</h3>
      <ul>
        <li>After the last block, the model produces predictions.</li>
        <li>Each final vector is passed through an <strong>unembedding matrix (U)</strong> and a <strong>softmax</strong> function to choose the most likely next token.</li>
      </ul>
    </section>


    <section>
      <h2>Summary</h2>
      <p>
        In short: a <strong>transformer</strong> is a neural network that predicts tokens one by one. It works by first turning tokens into vectors, then processing them through stacks of attention-based layers, and finally predicting the next token using a softmax output head.
      </p>
    </section>

  <h1>B. Attention</h2>
  <p>
    In older models like <b>word2vec</b>, each word always had <b>one fixed meaning</b>.  
    For example:
  </p>
  <ul>
    <li>The word <b>‚Äúchicken‚Äù</b> always had the same vector, no matter if we meant the animal or the food.</li>
    <li>The word <b>‚Äúit‚Äù</b> always had the same vector, even though in different sentences it can mean different things.</li>
  </ul>
  <p>
    But in real life, the meaning of a word depends on <b>context</b> (the other words around it).  
    That‚Äôs what <b>transformers + attention</b> solve.
  </p>

  <h3>üêî Example 1: "it" with different meanings</h3>
  <ol>
    <li><i>The chicken didn‚Äôt cross the road because <b>it</b> was too tired.</i> ‚Üí Here, <b>it = chicken</b>.</li>
    <li><i>The chicken didn‚Äôt cross the road because <b>it</b> was too wide.</i> ‚Üí Here, <b>it = road</b>.</li>
  </ol>
  <p>
    üëâ The word ‚Äúit‚Äù changes its meaning depending on what came before.  
    Transformers figure this out using <b>attention</b>: they look at all the words in the sentence and decide which ones are important for understanding ‚Äúit‚Äù.
  </p>

  <p><b>Attention</b> is the mechanism in the transformer that weighs and combines the
representations from appropriate other tokens in the context from layer k+1 to build
the representation for tokens in layer k.</p>

  <img src="./images/SimpleAttentionDiagram.png" 
       alt="Simple Attention Diagram" 
       width="700">

    <!-- Simple Attention Diagram (copy into your HTML) -->
<div style="max-width:900px;margin:24px auto;padding:18px;font-family:system-ui,Segoe UI,Roboto,Arial;">
  <h3 style="margin:0 0 12px">Attention diagram ‚Äî "it" attends to earlier tokens</h3>
  <svg viewBox="0 0 900 220" width="100%" height="220" aria-label="Simple attention diagram">
    <!-- Background -->
    <rect x="0" y="0" width="900" height="220" fill="transparent"></rect>

    <!-- Token boxes -->
    <!-- positions for tokens -->
    <!-- We'll place tokens horizontally -->
    <g font-family="Segoe UI, Roboto, Arial" font-size="14" fill="#111">
      <!-- token rectangles -->
      <rect x="20"  y="140" width="110" height="36" rx="6" fill="#f3f4f6" stroke="#cbd5e1"/>
      <text x="75"  y="165" text-anchor="middle" fill="#0f172a">The</text>

      <rect x="145" y="140" width="140" height="36" rx="6" fill="#fff8e1" stroke="#f59e0b"/>
      <text x="215" y="165" text-anchor="middle" fill="#92400e">chicken</text>

      <rect x="295" y="140" width="140" height="36" rx="6" fill="#f3f4f6" stroke="#cbd5e1"/>
      <text x="365" y="165" text-anchor="middle" fill="#0f172a">didn‚Äôt</text>

      <rect x="445" y="140" width="120" height="36" rx="6" fill="#f3f4f6" stroke="#cbd5e1"/>
      <text x="505" y="165" text-anchor="middle" fill="#0f172a">cross</text>

      <rect x="580" y="140" width="100" height="36" rx="6" fill="#f3f4f6" stroke="#cbd5e1"/>
      <text x="630" y="165" text-anchor="middle" fill="#0f172a">the</text>

      <rect x="695" y="140" width="120" height="36" rx="6" fill="#fff8e1" stroke="#f59e0b"/>
      <text x="755" y="165" text-anchor="middle" fill="#92400e">road</text>

      <rect x="20"  y="60"  width="110" height="36" rx="6" fill="#f3f4f6" stroke="#cbd5e1"/>
      <text x="75"  y="85"  text-anchor="middle" fill="#0f172a">because</text>

      <rect x="145" y="60"  width="140" height="36" rx="6" fill="#f3f4f6" stroke="#cbd5e1"/>
      <text x="215" y="85"  text-anchor="middle" fill="#0f172a">it</text>

      <rect x="295" y="60"  width="140" height="36" rx="6" fill="#f3f4f6" stroke="#cbd5e1"/>
      <text x="365" y="85"  text-anchor="middle" fill="#0f172a">was</text>

      <rect x="445" y="60"  width="120" height="36" rx="6" fill="#f3f4f6" stroke="#cbd5e1"/>
      <text x="505" y="85"  text-anchor="middle" fill="#0f172a">too</text>

      <rect x="580" y="60"  width="160" height="36" rx="6" fill="#f3f4f6" stroke="#cbd5e1"/>
      <text x="660" y="85"  text-anchor="middle" fill="#0f172a"> wide</text>
    </g>

    <!-- Attention arrows from "it" (at 215,85 center) to earlier tokens -->
    <!-- We'll draw curved paths with stroke-width proportional to weight -->
    <!-- Weight example: chicken 0.6, road 0.3, others small 0.05 -->
    <defs>
      <marker id="arrowhead" markerWidth="6" markerHeight="6" refX="6" refY="3"
              orient="auto" markerUnits="strokeWidth">
        <path d="M0,0 L6,3 L0,6 z" fill="#0ea5e9"></path>
      </marker>
      <g id="legend">
        <rect x="740" y="10" width="140" height="72" rx="6" fill="#ffffff" stroke="#e6e9ef"/>
        <text x="752" y="30" fill="#0f172a" font-size="13" font-weight="600">Legend</text>
        <line x1="752" y1="44" x2="792" y2="44" stroke="#0ea5e9" stroke-width="6" stroke-linecap="round"/>
        <text x="800" y="48" fill="#334155" font-size="12">strong attention</text>
        <line x1="752" y1="58" x2="792" y2="58" stroke="#0ea5e9" stroke-width="3" stroke-linecap="round"/>
        <text x="800" y="62" fill="#334155" font-size="12">weaker attention</text>
      </g>
    </defs>

    <!-- Strong attention: it -> chicken -->
    <path d="M215,75 C200,40 220,20 215,165" fill="none" stroke="#0ea5e9" stroke-linecap="round"
          stroke-width="3.5" opacity="0.95" marker-end="url(#arrowhead)"/>
    <!-- Annotate weight -->
    <text x="180" y="45" fill="#0f172a" font-size="12" font-family="Segoe UI, Roboto, Arial">0.30</text>

    <!-- Medium attention: it -> road -->
    <path d="M215,75 C330,30 430,30 755,140" fill="none" stroke="#0ea5e9" stroke-linecap="round"
          stroke-width="6" opacity="0.9" marker-end="url(#arrowhead)"/>
    <text x="500" y="30" fill="#0f172a" font-size="12">0.60</text>

    <!-- Small attention: it -> 'the' (example small) -->
    <path d="M215,75 C225,90 630,120 640,160" fill="none" stroke="#0ea5e9" stroke-linecap="round"
          stroke-width="1.2" opacity="0.6" marker-end="url(#arrowhead)"/>
    <text x="590" y="120" fill="#475569" font-size="11">0.05</text>

    <!-- Place legend -->
    <use href="#legend" x="0" y="0"/>
  </svg>

  <p style="margin:10px 0 0;color:#334155;font-size:14px;">
    The diagram above shows how the token <strong>it</strong> (upper row) assigns attention to earlier tokens.
    Thickness of the arrow indicates attention weight (stronger ‚Üí thicker). Here <em>chicken</em> receives the
    highest attention (0.60), <em>road</em> gets some attention (0.30), and other nearby tokens receive small weights.
  </p>
</div>

  <h3>‚è≥ Example 2: Reading step by step</h3>
  <p>
    When reading left to right:
  </p>
  <p><i>The chicken didn‚Äôt cross the road because it...</i></p>
  <p>
    At this moment, we <b>don‚Äôt yet know</b> if ‚Äúit‚Äù refers to chicken or road.  
    So the model may <b>pay attention to both</b> until the next word (‚Äútired‚Äù or ‚Äúwide‚Äù) makes it clear.
  </p>

  <h3>üîë Example 3: Grammar & Meaning</h3>
  <ol>
    <li>
      <i>The keys to the cabinet are on the table.</i><br>
      Subject = <b>keys</b> (plural).<br>
      Verb = <b>are</b> (plural).<br>
      Attention helps the model connect <b>keys</b> with <b>are</b>, even though <b>cabinet</b> is closer.
    </li>
    <li>
      <i>I walked along the pond, and noticed one of the trees along the bank.</i><br>
      ‚Äúbank‚Äù here means <b>river bank</b>, not <b>financial bank</b>.<br>
      The model knows this because of nearby words like <b>pond</b> and <b>trees</b>.
    </li>
  </ol>

  <h3>üéØ How Attention Works</h3>
  <p>
    At each <b>layer</b> of the transformer:
  </p>
  <ul>
    <li>The model takes a word (say ‚Äúit‚Äù)</li>
    <li>Looks at <b>all other words</b> in the sentence</li>
    <li>Assigns <b>attention weights</b> (higher = more important)</li>
    <li>Builds a new <b>contextual meaning</b> for ‚Äúit‚Äù</li>
  </ul>
  <p>
    In the diagram, the word <b>‚Äúit‚Äù</b> attends strongly to <b>chicken</b> and <b>road</b> because these are the most likely references.
  </p>

  <h3>‚úÖ In short:</h3>
  <ul>
    <li>Transformers don‚Äôt just see a word ‚Üí they <b>look at all other words</b> to guess its meaning.</li>
    <li>That‚Äôs why ‚Äúit‚Äù can mean chicken in one case and road in another.</li>
    <li>This is called <b>self-attention</b>: each word attends to others in the sentence.</li>
  </ul>


    <section>
      <h2>What attention computes</h2>
      <p><b><i>‚ÄúWhen I‚Äôm looking at the current word, which earlier words should I pay more attention to, and by how much?‚Äù</p>

      <p>It does this by giving weights (importance values) to all the earlier words and then combining them into a new representation.</i></b></p>
      <p>
        At a given layer of a transformer, attention builds a new vector representation for each token by selectively combining information from earlier tokens. For token position <span class="math">i</span>, attention takes the current representation <code>x<sub>i</sub></code> and a set of prior representations <code>x<sub>1</sub>, ‚Ä¶, x<sub>i</sub></code> and produces a new vector <code>a<sub>i</sub></code> that summarizes the context most relevant to <code>x<sub>i</sub></code>.
      </p>

      <p>
        In causal (left-to-right) language models the context for position <em>i</em> is the tokens up to and including position <em>i</em> (no future tokens). Attention is computed independently at every position, so a self-attention layer maps the full input sequence <code>(x‚ÇÅ,‚Ä¶,x‚Çô)</code> to an output sequence <code>(a‚ÇÅ,‚Ä¶,a‚Çô)</code> of the same length.
      </p>
    </section>

    <section>
      <h2>Intuition</h2>
      <p>
        Attention answers the question: <em>‚ÄúGiven the current token, which earlier tokens are most useful for understanding it?‚Äù</em>  
        It does so by (1) scoring how similar each earlier token is to the current token, (2) turning those scores into normalized weights (probabilities), and (3) computing a weighted sum of the earlier token vectors using those weights.
      </p>
    </section>

    <section>
      <h2>Step-by-step ‚Äî the simplified attention formula</h2>

      <h3>Step 1 ‚Äî inputs</h3>
      <p>
        We assume token vectors <code>x‚ÇÅ, x‚ÇÇ, ‚Ä¶, x‚Çô</code>. For the current position <em>i</em> we will produce <code>a<sub>i</sub></code>, the attention output for position <em>i</em>.
      </p>

      <h3>Step 2 ‚Äî raw similarity scores (dot product)</h3>
      <p>
        For each earlier position <code>j ‚â§ i</code> compute a scalar score measuring similarity between <code>x<sub>i</sub></code> and <code>x<sub>j</sub></code>. The simplest choice is the dot product:
      </p>
      <pre><code class="math">score(x_i, x_j) = x_i ¬∑ x_j</code></pre>
      <p>
        Intuition: if two vectors point in similar directions, their dot product is large ‚Äî that suggests the earlier token <code>x<sub>j</sub></code> is relevant to <code>x<sub>i</sub></code>.
      </p>

      <h3>Step 3 ‚Äî normalize scores to weights (softmax)</h3>
      <p>
        Raw scores can be any real numbers. We convert them into a probability distribution over the earlier tokens so the values become interpretable as "how much to use each token". This is done with the softmax:
      </p>
      <pre><code class="math">a_{ij} = softmax_j( score(x_i, x_j) )  for j ‚â§ i</code></pre>
      <p>
        The result <code>a<sub>ij</sub></code> is nonnegative and the weights for all <code>j ‚â§ i</code> sum to 1. Typically the weight on <code>j = i</code> (the token itself) is large, but other tokens may also receive substantial weight if they are similar to <code>x<sub>i</sub></code>.
      </p>

      <h3>Step 4 ‚Äî weighted sum</h3>
      <p>
        Use the weights to combine the earlier vectors into the attention output:
      </p>
      <pre><code class="math">a_i = Œ£_{j ‚â§ i} a_{ij} ¬∑ x_j</code></pre>
      <p>
        In words: multiply each prior vector <code>x<sub>j</sub></code> by its attention weight <code>a<sub>ij</sub></code> and add them up. The result <code>a<sub>i</sub></code> is a new contextualized vector for position <em>i</em>.
      </p>
    </section>

    <section>
      <h2>Why these steps make sense</h2>
      <ol>
        <li><strong>Dot product ‚Üí similarity:</strong> vectors that are similar point in similar directions; the dot product is a direct measure of that.</li>
        <li><strong>Softmax ‚Üí normalized importance:</strong> exponent + normalization produces positive weights that sum to one, so the model forms a convex combination (a weighted average) of prior vectors.</li>
        <li><strong>Weighted sum ‚Üí contextual vector:</strong> the output <code>a<sub>i</sub></code> is a blended vector that pulls information from the tokens that matter most for the current position.</li>
      </ol>

      <p>
        In language, this lets a token's representation incorporate information from words that might be far away in the sentence: pronoun resolution, agreement (subject ‚Üî verb), and disambiguating word senses are all examples where attention helps.
      </p>
    </section>

    <section>
      <h2>Summary</h2>
      <p>
        Attention converts raw similarity scores between the current token and each prior token into a probability distribution (softmax), then builds a context-aware vector as the weighted sum of prior vectors. This simple mechanism‚Äîscore, normalize, mix‚Äîrepeats at every layer and every position, and is the key building block that lets transformers form rich contextualized token embeddings.
      </p>
    </section>


<!-- Detailed Step-by-Step Attention Diagram -->
  <h2 style="margin:0 0 12px;">Attention ‚Äî A Step-by-Step Example</h2>

  <p>
    Let‚Äôs carefully walk through how <b>self-attention</b> works inside a transformer. 
    We‚Äôll use a short sentence: <i>‚ÄúThe cat sat‚Äù</i>.  
    Our focus will be on the token <b>‚Äúsat‚Äù</b> (x‚ÇÉ), and how the model builds its new 
    representation <b>a‚ÇÉ</b> by looking back at earlier tokens (<b>x‚ÇÅ = ‚Äúthe‚Äù</b>, 
    <b>x‚ÇÇ = ‚Äúcat‚Äù</b>, and itself).
  </p>

  <!-- The original SVG diagram -->
  <svg viewBox="0 0 820 320" width="100%" height="320" aria-label="Step-by-step attention diagram">
    <defs>
      <marker id="arrow" markerWidth="8" markerHeight="8" refX="7" refY="4"
              orient="auto" markerUnits="strokeWidth">
        <path d="M0,0 L8,4 L0,8 z" fill="#0ea5e9"></path>
      </marker>
      <style>
        .tok { fill:#fff8e1; stroke:#f59e0b; stroke-width:1.2; rx:8; }
        .tok-text { font-size:14px; fill:#92400e; font-weight:600; }
        .label { font-size:13px; fill:#0f172a; }
        .small { font-size:12px; fill:#334155; }
        .box { fill:#f8fafc; stroke:#e6eef6; stroke-width:1; rx:8; }
      </style>
    </defs>

    <!-- Token row (context) -->
    <g>
      <rect x="80"  y="50" width="120" height="40" class="tok"></rect>
      <text x="140" y="78" text-anchor="middle" class="tok-text">x‚ÇÅ ‚Äî the</text>

      <rect x="260" y="50" width="120" height="40" class="tok"></rect>
      <text x="320" y="78" text-anchor="middle" class="tok-text">x‚ÇÇ ‚Äî cat</text>

      <rect x="440" y="50" width="120" height="40" class="tok"></rect>
      <text x="500" y="78" text-anchor="middle" class="tok-text">x‚ÇÉ ‚Äî sat</text>
    </g>

    <!-- Current token (position i) shown below -->
    <g>
      <rect x="440" y="190" width="120" height="40" fill="#eef2ff" stroke="#60a5fa" rx="8"></rect>
      <text x="500" y="218" text-anchor="middle" class="label">current: x·µ¢ = x‚ÇÉ (sat)</text>
    </g>

    <!-- Example raw scores (dot products) -->
    <!-- scores chosen: [1.0, 3.0, 2.0] -->
    <!-- stroke widths proportional to softmax weights [0.09, 0.67, 0.24] -->

    <!-- arrow to x1 (weak) -->
    <path d="M500,210 C420,170 300,140 160,90"
          fill="none" stroke="#0ea5e9" stroke-linecap="round"
          stroke-width="1.3" opacity="0.9" marker-end="url(#arrow)"/>
    <text x="360" y="140" class="small">score = 1.00</text>
    <text x="330" y="155" class="small">weight ‚âà 0.09</text>

    <!-- arrow to x2 (strong) -->
    <path d="M500,210 C440,150 360,125 320,90"
          fill="none" stroke="#0ea5e9" stroke-linecap="round"
          stroke-width="9.3" opacity="0.95" marker-end="url(#arrow)"/>
    <text x="420" y="130" class="small">score = 3.00</text>
    <text x="420" y="145" class="small">weight ‚âà 0.67</text>

    <!-- arrow to x3 (medium/self) -->
    <path d="M500,210 C520,170 520,120 500,90"
          fill="none" stroke="#0ea5e9" stroke-linecap="round"
          stroke-width="3.4" opacity="0.95" marker-end="url(#arrow)"/>
    <text x="525" y="125" class="small">score = 2.00</text>
    <text x="525" y="140" class="small">weight ‚âà 0.24</text>

    <!-- Softmax box (right) -->
    <g>
      <rect x="610" y="40" width="180" height="120" class="box"></rect>
      <text x="700" y="64" text-anchor="middle" class="label">softmax</text>
      <text x="690" y="92" text-anchor="middle" class="small">softmax([1.0, 3.0, 2.0])</text>
      <text x="690" y="112" text-anchor="middle" class="small">‚Üí [0.09, 0.67, 0.24]</text>
    </g>

    <!-- Result box: weighted sum -->
    <g>
      <rect x="260" y="240" width="300" height="44" rx="8" fill="#f1f5f9" stroke="#cbd5e1"></rect>
      <text x="410" y="269" text-anchor="middle" class="label">
        a·µ¢ = 0.09¬∑x‚ÇÅ + 0.67¬∑x‚ÇÇ + 0.24¬∑x‚ÇÉ
      </text>
    </g>

    <!-- small explanatory footnote -->
    <text x="16" y="300" class="small">Step 1: compute dot scores (similarity). Step 2: softmax ‚Üí weights. Step 3: weighted sum ‚Üí a·µ¢.</text>
  </svg>

  <p style="margin:10px 0 0;color:#334155;font-size:14px;">
    This diagram shows one token (current x·µ¢ = <b>sat</b>) attending to earlier tokens x‚ÇÅ, x‚ÇÇ, x‚ÇÉ.
    Raw dot-product scores (1.0, 3.0, 2.0) are normalized by softmax to weights (~0.09, 0.67, 0.24),
    then the output a·µ¢ is the weighted sum of the earlier vectors.
  </p>

  <!-- Expanded explanation below the diagram -->
  <h3>Step 1 ‚Äî Compare with earlier tokens</h3>
  <p>
    The current word <b>‚Äúsat‚Äù</b> (x‚ÇÉ) is compared with each token before it 
    (including itself). These comparisons are done using the 
    <b>dot product</b>. For our example, the similarity scores come out as:
  </p>
  <ul>
    <li>x‚ÇÉ with x‚ÇÅ (‚Äúthe‚Äù): 1.0 ‚Üí weak similarity</li>
    <li>x‚ÇÉ with x‚ÇÇ (‚Äúcat‚Äù): 3.0 ‚Üí strong similarity</li>
    <li>x‚ÇÉ with x‚ÇÉ (‚Äúsat‚Äù): 2.0 ‚Üí medium similarity</li>
  </ul>

  <h3>Step 2 ‚Äî Convert scores into probabilities</h3>
  <p>
    Raw scores can be large or negative, so we normalize them using the 
    <b>softmax function</b>. This turns the scores into probabilities 
    that always add up to 1:
  </p>
  <pre style="background:#f8fafc;padding:8px;border-radius:6px;">
    softmax([1.0, 3.0, 2.0]) ‚Üí [0.09, 0.67, 0.24]
  </pre>
  <p>
    Meaning: the word ‚Äúsat‚Äù attends mostly to ‚Äúcat‚Äù (67%), some to itself (24%), 
    and very little to ‚Äúthe‚Äù (9%).
  </p>

  <h3>Step 3 ‚Äî Build the new representation</h3>
  <p>
    Finally, we compute the new output vector a‚ÇÉ by taking a 
    <b>weighted sum</b> of all the inputs:
  </p>
  <pre style="background:#f8fafc;padding:8px;border-radius:6px;">
    a‚ÇÉ = 0.09¬∑x‚ÇÅ + 0.67¬∑x‚ÇÇ + 0.24¬∑x‚ÇÉ
  </pre>
  <p>
    This means the new meaning of ‚Äúsat‚Äù now strongly includes information about 
    ‚Äúcat‚Äù (the subject), making it easier for the model to understand 
    <i>who did the action</i>.
  </p>

  <h3>‚úÖ Key takeaway</h3>
  <p>
    Attention doesn‚Äôt just look at the current word in isolation. Instead, it asks: 
    <i>‚ÄúWhich earlier words matter most for understanding this one?‚Äù</i>.  
    The answer is encoded in the attention weights, and the new vector a·µ¢ 
    is built from a blend of those important words.
  </p>


<h2> Understanding a Single Attention Head</h2>


<p>
The concept of <strong>attention</strong> lies at the very heart of the Transformer architecture. In this chapter, we will explore step by step how a single attention head works. Rather than thinking of it as an abstract equation, we will break it down into an intuitive process. Imagine that every word in a sentence has the ability to ask: <em>‚ÄúWhom should I pay attention to, and by how much?‚Äù</em>
</p>


<h2>Step 1: Input Representation</h2>
<p>
We begin with an input embedding for each token in the sequence. If the model dimension is <em>d</em>, each token is represented as a vector:
</p>
<div class="equation">
x<sub>i</sub> ‚àà ‚Ñù<sup>1 √ó d</sup>
</div>
<p>
Here, <em>x<sub>i</sub></em> is the embedding of the <em>i</em>-th word.
</p>


<h2>Step 2: Creating Queries, Keys, and Values</h2>
<p>
From each input vector, the model creates three different projections: a <strong>Query (Q)</strong>, a <strong>Key (K)</strong>, and a <strong>Value (V)</strong>. These are obtained by multiplying the input with learned weight matrices:
</p>
<div class="equation">
q<sub>i</sub> = x<sub>i</sub> W<sub>Q</sub>, &nbsp;
k<sub>i</sub> = x<sub>i</sub> W<sub>K</sub>, &nbsp;
v<sub>i</sub> = x<sub>i</sub> W<sub>V</sub>
</div>
<p>
The role of these projections is to allow the same word to play different parts in the attention process: the <em>query</em> asks the question, the <em>key</em> provides the address to be matched, and the <em>value</em> carries the information to be retrieved.
</p>


<h2>Step 3: Matching Queries with Keys</h2>
<p>
Once we have queries and keys, we can measure how strongly a word should attend to others. This is done by taking the dot product between a query and a key, scaled by the dimension of the key vectors:
</p>
<div class="equation">
score(x<sub>i</sub>, x<sub>j</sub>) = (q<sub>i</sub> ¬∑ k<sub>j</sub>) / ‚àöd<sub>k</sub>
</div>
<p>
If the dot product is large, the query and key are similar, which means the word <em>x<sub>i</sub></em> should pay closer attention to <em>x<sub>j</sub></em>.
</p>


<h2>Step 4: From Scores to Attention Weights</h2>
<p>
These raw scores are then normalized using the <strong>softmax</strong> function. This ensures that the attention weights form a probability distribution:
</p>
<div class="equation">
a<sub>ij</sub> = softmax(score(x<sub>i</sub>, x<sub>j</sub>))
</p>


  <p>
    Attention is a key concept in modern deep learning architectures such as the Transformer.
    At its core, the attention mechanism allows each token in a sequence to look at, or ‚Äúattend to,‚Äù
    other tokens in order to build a more context-aware representation.
  </p>

  <p>
    Let us break down the process step by step. Suppose we have an input sequence of tokens.
    Each token <em>x<sub>i</sub></em> is first projected into three different spaces:
    a Query (Q), a Key (K), and a Value (V). These are obtained through learned linear transformations.
  </p>

  <p>
    The Query of the current token is compared with the Keys of all tokens. This comparison is done
    by taking dot products, which produce similarity scores. These scores measure how much focus
    should be given to each token relative to the current one.
  </p>

  <p>
    The similarity scores are then normalized using the <strong>softmax</strong> function,
    converting them into attention weights. These weights represent how strongly each token contributes
    to the final representation of the current token.
  </p>

  <p>
    Using these attention weights, we take a weighted sum of the Value vectors. This results in
    a new vector representation for the current token ‚Äî one that incorporates contextual information
    from the entire sequence.
  </p>

  <p>
    Finally, this output is passed through another learned linear transformation, represented
    by <em>W<sub>O</sub></em>, to bring it back into the same dimension as the input embeddings.
  </p>

  <div class="diagram-container">
    <svg viewBox="0 0 1280 380" role="img" aria-label="Block diagram of a single attention head">
      <!-- Input -->
      <rect x="20" y="150" width="100" height="50" fill="#a3c9f1" stroke="#333" rx="6"/>
      <text x="70" y="180" text-anchor="middle" font-size="14" fill="#000">Input x·µ¢</text>

      <!-- Q, K, V projections -->
      <rect x="180" y="50" width="100" height="40" fill="#f9d29d" stroke="#333" rx="6"/>
      <text x="230" y="75" text-anchor="middle" font-size="14">Query (Q)</text>

      <rect x="180" y="150" width="100" height="40" fill="#f9d29d" stroke="#333" rx="6"/>
      <text x="230" y="175" text-anchor="middle" font-size="14">Key (K)</text>

      <rect x="180" y="250" width="100" height="40" fill="#f9d29d" stroke="#333" rx="6"/>
      <text x="230" y="275" text-anchor="middle" font-size="14">Value (V)</text>

      <!-- Arrows from input -->
      <line x1="120" y1="175" x2="180" y2="70" stroke="#333" marker-end="url(#arrow)"/>
      <line x1="120" y1="175" x2="180" y2="170" stroke="#333" marker-end="url(#arrow)"/>
      <line x1="120" y1="175" x2="180" y2="270" stroke="#333" marker-end="url(#arrow)"/>

      <!-- Dot product -->
      <rect x="340" y="50" width="120" height="40" fill="#d1f7c4" stroke="#333" rx="6"/>
      <text x="400" y="75" text-anchor="middle" font-size="14">Q ¬∑ K·µÄ</text>
      <line x1="280" y1="70" x2="340" y2="70" stroke="#333" marker-end="url(#arrow)"/>
      <line x1="280" y1="170" x2="340" y2="70" stroke="#333" marker-end="url(#arrow)"/>

      <!-- Softmax -->
      <rect x="500" y="50" width="120" height="40" fill="#ffd6e0" stroke="#333" rx="6"/>
      <text x="560" y="75" text-anchor="middle" font-size="14">Softmax</text>
      <line x1="460" y1="70" x2="500" y2="70" stroke="#333" marker-end="url(#arrow)"/>

      <!-- Weighted sum -->
      <rect x="680" y="150" width="140" height="40" fill="#e0c7ff" stroke="#333" rx="6"/>
      <text x="750" y="175" text-anchor="middle" font-size="14">Œ£ (weights √ó V)</text>
      <line x1="620" y1="70" x2="680" y2="170" stroke="#333" marker-end="url(#arrow)"/>
      <line x1="280" y1="270" x2="680" y2="170" stroke="#333" marker-end="url(#arrow)"/>

      <!-- Output -->
      <rect x="900" y="150" width="120" height="50" fill="#a3f1d1" stroke="#333" rx="6"/>
      <text x="960" y="180" text-anchor="middle" font-size="14">Output</text>
      <line x1="820" y1="170" x2="900" y2="175" stroke="#333" marker-end="url(#arrow)"/>

      <!-- Arrow marker -->
      <defs>
        <marker id="arrow" markerWidth="10" markerHeight="10" refX="10" refY="5" orient="auto" markerUnits="strokeWidth">
          <path d="M0,0 L10,5 L0,10 z" fill="#333"/>
        </marker>
      </defs>
    </svg>
  </div>
  
    <img src="./images/attention.png" 
       alt="attention" 
       width="100%">
	   
	<img src="./images/attention2.png" 
       alt="attention" 
       width="100%">

  <div class="card" role="article" aria-labelledby="summary-title">
    <h3 id="summary-title">Summary ‚Äî Single Attention Head</h3>

    <div class="step">
      <p><strong>Each token</strong> <em>x<sub>i</sub></em> creates three versions of itself: <strong>Query (Q)</strong>, <strong>Key (K)</strong>, and <strong>Value (V)</strong>.</p>
    </div>

    <div class="step">
      <p><strong>Compare:</strong> The <em>query</em> of the current token is compared with the <em>keys</em> of all tokens to produce similarity scores.</p>
    </div>

    <div class="step">
      <p><strong>Normalize:</strong> The raw similarity scores are converted into attention weights using a <strong>softmax</strong>, so they form a probability distribution (weights sum to 1).</p>
    </div>

    <div class="step">
      <p><strong>Mix:</strong> These attention weights are used to compute a weighted sum of the <em>value</em> vectors, producing a new contextual representation (the attended output).</p>
    </div>

    <div class="step">
      <p><strong>Project back:</strong> The head output is reshaped back to the model dimension using the output matrix <span class="math">W<sub>O</sub></span>, so the final output has the same size as the original token vector.</p>
    </div>

    <p class="note">In compact form: each x<sub>i</sub> ‚Üí (Q,K,V) ‚Üí scores via Q¬∑K ‚Üí softmax ‚Üí weights ‚Üí weighted sum of V ‚Üí final projection with <span class="math">W<sub>O</sub></span>.</p>
  </div>
  
  
    <h2>Multi-Head Attention</h2>
	
	<h2>Intuition</h2>
    <p>
      Multi-head attention enriches the model‚Äôs representation by allowing it to examine
      the context from several different perspectives simultaneously. While one head may
      align strongly with semantically related words, another may emphasize positional
      structure, and another may attend to rare but important connections. The combination
      of all these heads, followed by a projection back to the model space, gives the
      transformer both breadth and depth in capturing dependencies across a sequence.
    </p>

    <p>
      A transformer does not rely on a single attention mechanism. Instead, it uses
      <strong>multiple parallel attention heads</strong> within the same layer. The idea is
      that each head can focus on a different aspect of the context. One head may capture
      short-range dependencies, another may focus on long-distance relationships, while
      yet another may specialize in syntactic or semantic cues. By combining these heads,
      the model gains a richer and more flexible representation of the input sequence.
    </p>

    <h2>Head-Specific Projections</h2>
    <p>
      Each attention head has its own set of learnable parameters. Given an input vector
      <code>x·µ¢</code> at position <code>i</code>, the model projects it into separate
      <em>query</em>, <em>key</em>, and <em>value</em> vectors for each head. For head
      <code>c</code>, this is written as:
    </p>
    <pre><code>q·∂ú·µ¢ = x·µ¢ ¬∑ W_Q·∂ú
k·∂ú‚±º = x‚±º ¬∑ W_K·∂ú
v·∂ú‚±º = x‚±º ¬∑ W_V·∂ú</code></pre>
    <p>
      Here, <code>W_Q·∂ú</code>, <code>W_K·∂ú</code>, and <code>W_V·∂ú</code> are parameter
      matrices of dimensions <code>d √ó d_k</code>, <code>d √ó d_k</code>, and
      <code>d √ó d_v</code> respectively. This means every head learns its own mapping from
      the model dimension <code>d</code> into smaller subspaces of size <code>d_k</code>
      and <code>d_v</code>.
    </p>

    <h2>Attention Within a Head</h2>
    <p>
      Once queries and keys are defined, the similarity between a query at position
      <code>i</code> and a key at position <code>j</code> is measured using the dot
      product. To stabilize gradients, this score is scaled by the square root of
      <code>d_k</code>:
    </p>
    <pre><code>score·∂ú(x·µ¢, x‚±º) = (q·∂ú·µ¢ ¬∑ k·∂ú‚±º) / ‚àö(d_k)</code></pre>
    <p>
      These scores are then normalized with a softmax across all context positions to
      obtain attention weights:
    </p>
    <pre><code>a·∂ú_{i j} = softmax(score·∂ú(x·µ¢, x‚±º))</code></pre>
    <p>
      Finally, the output of head <code>c</code> for position <code>i</code> is a weighted
      sum of the value vectors:
    </p>
    <pre><code>head·∂ú·µ¢ = Œ£‚±º a·∂ú_{i j} ¬∑ v·∂ú‚±º</code></pre>

    <h2>Combining Multiple Heads</h2>
    <p>
      Each head produces an output vector of size <code>1 √ó d_v</code>. If there are
      <code>A</code> heads, their results are concatenated to form a vector of size
      <code>1 √ó (A¬∑d_v)</code>. This combined vector is then projected back into the model
      dimension <code>d</code> using an additional matrix <code>W_O</code>:
    </p>
    <pre><code>a·µ¢ = ( head¬π·µ¢ ‚äï head¬≤·µ¢ ‚äï ‚Ä¶ ‚äï head·¥¨·µ¢ ) ¬∑ W_O</code></pre>
    <p>
      Here, ‚äï denotes concatenation. The matrix <code>W_O</code> has dimensions
      <code>(A¬∑d_v) √ó d</code>, ensuring the final multi-head output for each position
      returns to the expected model dimension.
    </p>
	
	<img src="./images/multiheadattention.png" 
       alt="multiheadattention" 
       width="100%">
	   
	<img src="./images/multiheadattention2.png" 
       alt="multiheadattention2" 
       width="100%">
  </div>

  <section>
    <h1>C. Transformer Block Overview</h1>
    <p>
      A <strong>transformer block</strong> is a modular unit used repeatedly in transformer models. Each block transforms the d-dimensional vector for one token in two main ways:
      <ol>
        <li>by letting the token <em>attend</em> to other tokens (self-attention),</li>
        <li>and by applying a position-wise feedforward network (FFN) to each token.</li>
      </ol>
      Complementing these are <strong>residual connections</strong> and <strong>layer normalization</strong>, which stabilize training and preserve information.
    </p>
  </section>

  <section>
    <h2>1. Residual Stream</h2>
    <p>
      Imagine a vertical pipe carrying a vector for a single token upward through the transformer's layers. Each component reads the current vector in the pipe, computes an output, and adds that output back into the pipe. This "residual stream" view emphasizes that information accumulates rather than being overwritten.
    </p>

    <div class="box">
      <h3>Why residuals?</h3>
      <p>Residual (skip) connections help gradients flow backward during training and guarantee the input signal remains available at every stage. Practically, they prevent the network from forgetting the original token embedding as deeper transforms are applied.</p>
    </div>

    <img src="./images/residualstream.png" 
       alt="Diagram of residualstream" 
       width="100%">

    <figure aria-label="Transformer block sketch">

      <figcaption style="font-size:12px;color:#444;margin-top:6px;">Figure: Simplified transformer block showing residual additions.</figcaption>
    </figure>

  </section>

  <section>
    <h2>2. Feedforward Network (FFN)</h2>
    <p>
      The feedforward network is <strong>applied independently to each token</strong>. It does not mix information across positions ‚Äî that is the job of attention. The FFN is identical across all positions but varies between layers (each layer has its own learned weights).
    </p>

    <h3>2.1 Mathematical form</h3>
    <p class="eq">The FFN for token <code>x<sub>i</sub></code> is typically written as:</p>
    <pre class="eq">FFN(x_i) = ReLU(x_i W_1 + b_1) W_2 + b_2</pre>

    <p>
      Here, <code>W_1</code> maps the d-dimensional token vector to a wider hidden dimension <code>d<sub>ff</sub></code>, and <code>W_2</code> maps back to the original dimension <code>d</code>. Commonly, <code>d<sub>ff</sub> &gt; d</code> (for example, 2048 vs 512). The intermediate ReLU (or GELU) gives the network nonlinearity.
    </p>

    <h3>2.2 Intuition</h3>
    <p>
      The FFN expands the representation into a higher-dimensional space where complex features can be computed, then compresses it back into the model dimension. Think of it as a per-token "processor" that performs richer transformations on that token's information after it has gathered context from attention.
    </p>
  </section>

  <section>
    <h2>3. Layer Normalization (LayerNorm)</h2>
    <p>
      LayerNorm standardizes the components of a single token vector so they have zero mean and unit variance, with learnable scale and shift. It is applied twice in each transformer block: once before attention and once before the feedforward network.
    </p>

    <h3>Step-by-step computation</h3>

<ol>
  <li>Mean: Œº = (1+2+3+4)/4 = 2.5.</li>
  <li>Variance: ((1‚àí2.5)¬≤ + (2‚àí2.5)¬≤ + (3‚àí2.5)¬≤ + (4‚àí2.5)¬≤)/4 = 1.25.</li>
  <li>Standard Deviation (Std): œÉ = ‚àö1.25 ‚âà 1.1180.</li>
  <li>Normalized vector: xÃÇ ‚âà [‚àí1.3416, ‚àí0.4472, 0.4472, 1.3416].</li>
  <li>
    Finally, add two learnable parameters:
    <ul>
      <li><strong>Œ≥</strong> (scaling factor, or ‚Äúgain‚Äù)</li>
      <li><strong>Œ≤</strong> (shifting factor, or ‚Äúoffset‚Äù)</li>
    </ul>
    The final formula is:<br>
    <code>LayerNorm(x) = Œ≥ ¬∑ xÃÇ + Œ≤</code><br>
    This keeps the vector centered around 0 with unit variance, but also allows the network 
    to adjust the scale and shift using Œ≥ and Œ≤.
  </li>
</ol>


    <div class="note">Note: LayerNorm operates across the <em>features</em> of a single token (the d components), not across tokens in a batch. This differs from BatchNorm, which normalizes across a batch dimension.</div>
  </section>

  <section>
    <h2>4. Full transformer block ‚Äî step-by-step equations</h2>
    <p>Below is the canonical sequence of operations for a single token <code>x<sub>i</sub></code> inside one transformer block:</p>
    <pre class="eq">t1_i = LayerNorm(x_i)
t2_i = MultiHeadAttention(t1_i, [t1_1, ..., t1_N])
t3_i = t2_i + x_i
t4_i = LayerNorm(t3_i)
t5_i = FFN(t4_i)
h_i  = t5_i + t3_i</pre>

    <p>
      - <code>MultiHeadAttention(¬∑)</code> is the component that mixes information across tokens (every token's <code>t1</code> vectors are available to the attention mechanism). 
      - The two additions (<code>+ x_i</code> and <code>+ t3_i</code>) are residual connections that preserve earlier representations.
    </p>
  </section>

  <section>
    <h2>5. Where does cross-token information enter?</h2>
    <p>
      Only the attention mechanism reads other token streams. Attention pulls information from other residual streams (other token positions) and writes its result back into the current token's stream via the residual addition. FFN and LayerNorm act only on the local token vector.
    </p>

    <div class="box">
      <h3>Elhage et al. (residual movement)</h3>
      <p>In their analysis, attention heads can be viewed as literally moving pieces of information from one token's residual stream to another token's stream. This means the final vector at a position can contain subspaces encoding other tokens' content.</p>
    </div>
  </section>

      <img src="./images/crosstoken.png" 
       alt="Diagram of crosstoken" 
       width="400">
    <figure aria-label="Transformer block crosstoken">

      <figcaption style="font-size:12px;color:#444;margin-top:6px;">Figure: An attention head can move information from token A‚Äôs residual stream into
token B‚Äôs residual stream..</figcaption>
    </figure>
  

  <section>
    <h2>6. Stacking blocks ‚Äî building deep models</h2>
    <p>
      Transformer models are created by stacking many identical blocks. Because each block's input and output dimensions match (both are <code>d</code>), stacking is straightforward. Typical layer counts:
    </p>
    <table>
      <thead><tr><th>Model</th><th>Typical layers</th></tr></thead>
      <tbody>
        <tr><td>T5 / GPT-3 small</td><td>~12</td></tr>
        <tr><td>GPT-3 large</td><td>~96</td></tr>
        <tr><td>Modern LLMs (varies)</td><td>100s or more</td></tr>
      </tbody>
    </table>

    <p>
      At shallow layers, the residual stream mostly represents the current token. At deeper layers, the stream often encodes information useful for predicting the <em>next</em> token ‚Äî this is a result of training objectives such as next-token prediction.
    </p>
  </section>

  <section>
    <h2>7. Practical notes and variations</h2>
    <ul>
      <li><strong>Activation function:</strong> Many modern implementations use GELU instead of ReLU inside the FFN.</li>
      <li><strong>Pre-LN vs Post-LN:</strong> The textbook equations above represent <em>pre-layer-norm</em> (LayerNorm before attention/FFN). Some older descriptions use post-layer-norm (LayerNorm after each residual add). Pre-LN tends to be more stable in deep networks.</li>
      <li><strong>Normalization details:</strong> Implementations add a small epsilon to the denominator when dividing by <code>\sigma</code> to avoid numerical issues.</li>
    </ul>
  </section>

  <section>
    <h2>8. Summary</h2>
    <p>
      Each transformer block:
      <ul>
        <li>normalizes the token vector,</li>
        <li>allows the token to gather context via multi-head attention,</li>
        <li>adds that context back using a residual connection,</li>
        <li>normalizes again,</li>
        <li>applies a position-wise feedforward network (FFN),</li>
        <li>and adds the FFN output back via another residual connection.</li>
      </ul>
    </p>
    <p>Stack many such blocks and you have the deep transformer architectures used for tasks in NLP, vision, and beyond.</p>
  </section>

    <h2>9. Parallelizing Transformer Computations with Matrices</h2>

  <p>
    So far, we have described the transformer block as if it were computing the output for 
    <strong>one token at a time</strong>. For example, in self-attention, we showed how a single 
    token vector \(x_i\) produces a query, key, and value, and how these interact with other tokens. 
    But in reality, transformers do not process tokens one by one. Instead, they make use of 
    <strong>parallel computation</strong>, which allows them to handle <em>all tokens at once</em>. 
    This parallelism is one of the main reasons transformers are so efficient and scalable.
  </p>

  <h2>1. Representing the Input as a Matrix</h2>

  <p>
    Imagine we have an input sequence with \(N\) tokens. Each token is represented by an embedding 
    vector of dimension \(d\). Instead of treating each token separately, we can stack all token 
    vectors into a single matrix:
  </p>

  <div class="equation">
    \[
    X \in \mathbb{R}^{N \times d}
    \]
  </div>

  <p>
    Here:
  </p>
  <ul>
    <li>\(N\) = sequence length (number of tokens).</li>
    <li>\(d\) = embedding dimension (e.g., 512 or 1024 in practice).</li>
    <li>Each row of \(X\) corresponds to one token's embedding.</li>
  </ul>

  <p>
    For example, if we have a sentence with 4 tokens and embeddings of size 3, our matrix looks like:
  </p>

  <div class="equation">
    \[
    X =
    \begin{bmatrix}
    x_{1,1} & x_{1,2} & x_{1,3} \\
    x_{2,1} & x_{2,2} & x_{2,3} \\
    x_{3,1} & x_{3,2} & x_{3,3} \\
    x_{4,1} & x_{4,2} & x_{4,3}
    \end{bmatrix}
    \]
  </div>

  <p>
    This matrix representation lets us apply powerful matrix multiplication routines to all tokens 
    at once, instead of looping through them individually.
  </p>

  <h2>2. Parallelizing Attention (Single Head)</h2>

  <p>
    Recall that for each token we compute three vectors:
    <strong>Query (Q)</strong>, <strong>Key (K)</strong>, and <strong>Value (V)</strong>.
  </p>

  <p>
    For a single token, the computation is:
  </p>

  <div class="equation">
    \[
    q_i = x_i W_Q, \quad k_i = x_i W_K, \quad v_i = x_i W_V
    \]
  </div>

  <p>
    Instead of computing these for each token separately, we multiply the entire matrix \(X\) by 
    the projection matrices:
  </p>

  <div class="equation">
    \[
    Q = X W_Q, \quad K = X W_K, \quad V = X W_V
    \]
  </div>

  <p>
    Now:
  </p>
  <ul>
    <li>\(Q, K \in \mathbb{R}^{N \times d_k}\)</li>
    <li>\(V \in \mathbb{R}^{N \times d_v}\)</li>
  </ul>

  <p>
    Each row of \(Q, K, V\) corresponds to the query, key, or value of one token.
  </p>

  <h2>3. Computing All Attention Scores</h2>

  <p>
    To know how much each token should attend to others, we compare queries and keys. For one 
    token, this is just a dot product:
  </p>

  <div class="equation">
    \[
    \text{score}(i, j) = \frac{q_i \cdot k_j}{\sqrt{d_k}}
    \]
  </div>

  <p>
    But in parallel, we can compute all scores at once using:
  </p>

  <div class="equation">
    \[
    QK^\top \in \mathbb{R}^{N \times N}
    \]
  </div>

  <p>
    This gives us a full attention score matrix, where entry \((i, j)\) is the similarity between 
    token \(i\)'s query and token \(j\)'s key.
  </p>
      <img src="./images/QK.png" 
       alt="Diagram of scores" 
       width="400">
    <figure aria-label="QK">

      <figcaption style="font-size:12px;color:#444;margin-top:6px;">Figure: The N √ó N QK·µÄ matrix showing how it computes all qi ¬∑ k j comparisons in a
single matrix multiple.</figcaption>

  <h2>4. Masking Future Tokens</h2>

  <p>
    In language modeling, a token must not look at tokens that come after it. Otherwise, predicting 
    the next word would be trivial. To prevent this, we use a <strong>mask</strong>.
  </p>

  <p>
    The mask \(M\) is defined as:
  </p>

  <ul>
    <li>\(M_{ij} = 0\) if \(j \leq i\) (allowed).</li>
    <li>\(M_{ij} = -\infty\) if \(j > i\) (disallowed).</li>
  </ul>

  <p>
    The masked score matrix becomes:
  </p>

  <div class="equation">
    \[
    \text{MaskedScores} = \frac{QK^\top}{\sqrt{d_k}} + M
    \]
  </div>

        <img src="./images/QKmask.png" 
       alt="Diagram of scores" 
       width="400">
    <figure aria-label="QK">

      <figcaption style="font-size:12px;color:#444;margin-top:6px;">Figure: The N √ó N QK·µÄ matrix showing the qi ¬∑ k j values, with the upper-triangle por-
tion of the comparisons matrix zeroed out (set to ‚àí‚àû, which the softmax will turn to zero).</figcaption>

  <p>
    After applying the softmax function, the forbidden positions become 0.
  </p>

  <h2>5. Producing the Weighted Sum</h2>

  <p>
    Once we have normalized attention weights, we compute the output for each token by taking 
    a weighted sum of the value vectors:
  </p>

  <div class="equation">
    \[
    \text{head} = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}} + M\right)V
    \]
  </div>

  <p>
    This gives us an output of shape \([N \times d_v]\), where each row is the updated representation 
    of a token.
  </p>

  <h2>6. Multi-Head Attention</h2>

  <p>
    A single head may not be sufficient to capture all relationships. That‚Äôs why transformers 
    use <strong>multi-head attention</strong>. Each head has its own projection matrices 
    \(W_{Q_i}, W_{K_i}, W_{V_i}\), and computes its own head:
  </p>

  <div class="equation">
    \[
    \text{head}_i = \text{softmax}\!\left(\frac{Q_i K_i^\top}{\sqrt{d_k}} + M\right)V_i
    \]
  </div>

  <p>
    Then, we concatenate all heads:
  </p>

  <div class="equation">
    \[
    \text{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_A) \in \mathbb{R}^{N \times (A d_v)}
    \]
  </div>

  <p>
    Finally, we project back to the original model dimension:
  </p>

  <div class="equation">
    \[
    \text{MultiHeadAttention}(X) = \text{Concat}(\text{head}_1, \dots, \text{head}_A) W_O
    \]
  </div>

      <img src="./images/MHAttentionParallel.png" 
       alt="Diagram of MHAttentionParallel" 
       width="100%">
    <figure aria-label="MHAttentionParallel">

      <figcaption style="font-size:12px;color:#444;margin-top:6px;">Figure: Schematic of the attention computation for a single attention head in parallel. The first row shows
the computation of the Q, K, and V matrices. The second row shows the computation of QKT, the masking
(the softmax computation and the normalizing by dimensionality are not shown) and then the weighted sum of
the value vectors to get the final attention vectors.</figcaption>

  <h2>7. Adding Feedforward and Residual Connections</h2>

  <p>
    After attention, the transformer block applies:
  </p>

  <ol>
    <li><strong>Residual connection:</strong> Add the original input back.</li>
    <li><strong>Layer normalization:</strong> Normalize the result.</li>
    <li><strong>Feedforward network:</strong> Process each token independently.</li>
  </ol>

  <p>
    In matrix form:
  </p>

  <div class="equation">
    \[
    O = X + \text{MultiHeadAttention}(\text{LayerNorm}(X))
    \]
    \[
    H = O + \text{FFN}(\text{LayerNorm}(O))
    \]
  </div>

  <p>
    The key point: both the input \(X\) and the output \(H\) have shape \([N \times d]\). This 
    makes it possible to <strong>stack many transformer blocks</strong> one after another.
  </p>

  <h2>8. Why Parallelization Matters</h2>

  <p>
    By parallelizing computations:
  </p>
  <ul>
    <li><strong>Efficiency:</strong> We avoid loops over tokens and instead rely on matrix multiplication, 
        which GPUs and TPUs can perform extremely fast.</li>
    <li><strong>Scalability:</strong> Transformers can handle thousands of tokens in a single pass.</li>
    <li><strong>Quadratic cost:</strong> Computing \(QK^\top\) requires \(O(N^2)\) operations. This becomes 
        expensive for very long inputs, which is why researchers design special architectures for 
        <em>long-context transformers</em>.</li>
  </ul>

  <p>
    In short: transformers achieve their power not just from the attention mechanism, but also from 
    the clever use of <strong>parallel computation</strong> via matrix operations.
  </p>

  <h1>D. The Language Modeling Head in Transformers</h1>
  <section id="languagemodelinghead">
  <h2>1. Introduction</h2>
  <p>
    Transformers by themselves are general-purpose sequence models: they take a sequence of token embeddings,
    pass them through stacked layers of self-attention and feedforward networks, and produce contextualized hidden
    representations for each token. These hidden states capture rich information about both the input token and its
    surrounding context.
  </p>

  <p>
    However, the Transformer backbone does not inherently know how to predict words. To use Transformers for
    language modeling‚Äîthat is, assigning probabilities to sequences of words‚Äîwe need an additional component on
    top of the backbone: the <em>language modeling head</em>.
  </p>

  <p>
    This head converts the final hidden representation of each token into a probability distribution over the
    vocabulary, enabling the model to predict the next token, fill in masked tokens, or generate text.
  </p>
</section>

<h2>2. Language Models as Word Predictors</h2>
<p>
Language models are fundamentally <i>word predictors</i>. Given a sequence of words, they estimate the probability of the next word. For example, given the context:
</p>

<div class="equation">
P(fish | Thanks for all the)
</div>

<p>
A language model computes a conditional probability distribution over all possible next words. Earlier approaches such as <b>n-gram models</b> used counts of word 
occurrences within a fixed context of size <i>n ‚àí 1</i>. With transformers, however, the context is defined by the <i>attention window</i>, which can be very large 
(e.g., 32K tokens, or even millions with long-context architectures).
</p>

<h2>3. Architecture of the Language Modeling Head</h2>
<p>
The goal of the language modeling head is to take the hidden state output from the final transformer layer and transform it into a probability distribution 
over the vocabulary.
</p>

  <img src="./images/languagemodelinghead.png" 
       alt="Diagram of language modeling head" 
       width="100%">
    <figure aria-label="languagemodelinghead">

      <figcaption style="font-size:12px;color:#444;margin-top:6px;">Figure: The language modeling head: the circuit at the top of a transformer that maps from the output
embedding for token N from the last transformer layer (h<sup>L</sup><sub>N</sub> ) to a probability distribution over words in the
vocabulary V ..</figcaption>

<h3>3.1 Input Representation</h3>
<p>
Let <span class="equation">h<sup>L</sup><sub>N</sub> ‚àà ‚Ñù<sup>d</sup></span> 
denote the hidden state vector for the last token at position <i>N</i> from the final transformer block <i>L</i>. This vector encodes the context up to position <i>N</i>.
</p>

<h3>3.2 Linear Projection</h3>
<p>
The first step is to map this hidden state into a <i>logit vector</i> of vocabulary size <i>|V|</i>. This is done with a linear transformation:
</p>

<div class="equation">
u = h<sup>L</sup><sub>N</sub> W<sup>T</sup> + b
</div>

<p>
Here:
</p>
<ul>
  <li><i>W</i> ‚àà ‚Ñù<sup>|V| √ó d</sup> is the projection weight matrix,</li>
  <li><i>b</i> ‚àà ‚Ñù<sup>|V|</sup> is the bias vector,</li>
  <li><i>u</i> ‚àà ‚Ñù<sup>|V|</sup> is the logit vector, containing one score per token.</li>
</ul>

<h3>3.3 Weight Tying and the Unembedding Layer</h3>
<p>
Instead of learning a new projection matrix, modern models often use  <b>weight tying</b>. The same embedding matrix <i>E</i> that maps input tokens 
to embeddings is reused (via transposition) to map hidden states back  to vocabulary space. That is:
</p>

<ul>
  <li><i>E</i> ‚àà ‚Ñù<sup>|V| √ó d</sup> is the embedding matrix,</li>
  <li><i>E<sup>T</sup></i> ‚àà ‚Ñù<sup>d √ó |V|</sup> serves as the <i>unembedding</i> layer.</li>
</ul>

<p>
Thus the logits are computed as:
</p>

<div class="equation">
u = h<sup>L</sup><sub>N</sub> E<sup>T</sup>
</div>

<p>
This design reduces parameters and ensures that embeddings are optimized  both for input and output mappings.
</p>

<h3>3.4 Softmax to Probabilities</h3>
<p>
The logits are converted into probabilities over the vocabulary using  the softmax function:
</p>

<div class="equation">
y = softmax(u)
</div>

<p>
where <i>y</i> is a probability distribution of size <i>|V|</i>.
</p>

<h2>4. Applications of the LM Head</h2>

<h3>4.1 Sequence Probability</h3>
<p>
The probability distribution can be used to compute the likelihood of entire sequences, which is useful in tasks like perplexity evaluation.
</p>

<h3>4.2 Text Generation</h3>
<p>
For generation, we sample tokens from <i>y</i>. Options include:
</p>
<ul>
  <li><b>Greedy decoding:</b> choose the highest probability word.</li>
  <li><b>Sampling:</b> draw tokens stochastically, possibly with techniques        like temperature scaling, top-k, or nucleus sampling.</li>
</ul>

<p>
The chosen token is then fed back as input to predict subsequent tokens.
</p>

<h2>5. The Logit Lens</h2>
<p>
The <b>logit lens</b> (Nostalgebraist, 2020) is a diagnostic tool for interpretability.  By applying the unembedding matrix <i>E<sup>T</sup></i> to hidden states at intermediate 
layers (not just the final layer), we can observe approximate distributions  over the vocabulary. Although these internal states were not explicitly trained 
to represent predictions, the logit lens offers valuable insights into how  information evolves across layers.
</p>

<h2>6. Decoder-Only Transformers</h2>
<p>
Causal language models like GPT are often called <b>decoder-only transformers</b>.  This terminology reflects that they use only the decoder half of the original 
encoder‚Äìdecoder transformer architecture. In this setup:
</p>
<ul>
  <li>Masked self-attention ensures tokens cannot attend to future positions.</li>
  <li>The LM head produces the next-token distribution at each step.</li>
</ul>

  <img src="./images/decoderonly.png" 
       alt="Diagram of decoder only" 
       width="100%">
    <figure aria-label="decoderonly">

      <figcaption style="font-size:12px;color:#444;margin-top:6px;">Figure: A transformer language model (decoder-only), stacking transformer blocks
and mapping from an input token w<sub>i</sub> to to a predicted next token w<sub>i+1</sub>.</figcaption>

<h2>7. Summary</h2>
<p>
The language modeling head is the component that transforms abstract hidden 
representations into concrete predictions over words. Its steps are:
</p>
<ol>
  <li>Take the hidden state <i>h<sup>L</sup><sub>N</sub></i> from the final transformer layer.</li>
  <li>Project it into vocabulary space using a linear transformation (often tied to embeddings).</li>
  <li>Apply softmax to obtain probabilities <i>y</i>.</li>
  <li>Use these probabilities for evaluation or generation.</li>
</ol>

<p>
Together with the transformer backbone, the language modeling head enables  powerful generative models that define the state of the art in natural  language processing.
</p>


</body>
</html>
